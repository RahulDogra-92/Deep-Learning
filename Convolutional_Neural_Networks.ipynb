{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Networks",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulDogra-92/Deep-Learning-using-Pytorch/blob/main/Convolutional_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akC2DZ8ztOLN"
      },
      "source": [
        "#Convolutions in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0APAhBtrh71"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc6knaNEr5Jg"
      },
      "source": [
        "image = torch.rand(16,3,32,32)\n",
        "filter = torch.rand(1,3,5,5)\n",
        "out_feat_F = F.conv2d(image, filter, stride=1, padding=0)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux5sfyy-sXVU",
        "outputId": "83756450-5316-4d9c-d23e-0c36ee86fd76"
      },
      "source": [
        "print(out_feat_F.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIPr7ed1tkMj"
      },
      "source": [
        "#Convolution operator - Functional way\n",
        "how to build convolutional layers using functional package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgsKmFUesZ9i",
        "outputId": "a4203b2e-c769-4e78-95cb-deda1329c58d"
      },
      "source": [
        "# Create 10 random images\n",
        "image = torch.rand(10, 1, 28, 28)\n",
        "\n",
        "# Create 6 filters\n",
        "filters = torch.rand(6, 1, 3, 3)\n",
        "\n",
        "# Convolve the image with the filters\n",
        "output_feature = F.conv2d(image, filters, stride=1, padding=1)\n",
        "print(output_feature.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 6, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paRCQ2fBunbu"
      },
      "source": [
        "#Max-pooling in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th_vtzkZuMm9"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBOy2Guouwkv",
        "outputId": "9b4f8843-6992-43f2-eabc-52aa13043012"
      },
      "source": [
        "im = torch.Tensor([[[[3, 1, 3, 5], [6, 0, 7, 9],\n",
        "                     [3, 2, 1, 4], [0, 2, 4, 3]]]])\n",
        "\n",
        "output_feature_F = F.max_pool2d(im, 2)\n",
        "print(output_feature_F)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[6., 9.],\n",
            "          [3., 4.]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXsMO2w2vBnO",
        "outputId": "c65bba8c-8b69-449c-885c-39aa665fe945"
      },
      "source": [
        "im = torch.Tensor([[[[3, 1, 3, 5], [6, 0, 7, 9],\n",
        "                     [3, 2, 1, 4], [0, 2, 4, 3]]]])\n",
        "\n",
        "output_feature_F = F.avg_pool2d(im, 2)\n",
        "print(output_feature_F)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[2.5000, 6.0000],\n",
            "          [1.7500, 3.0000]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQpEjJhawCVW"
      },
      "source": [
        "#Max-pooling operator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjKi6ueNvXHo",
        "outputId": "bbbd5c27-24d4-4032-cdfe-30748f49c0f3"
      },
      "source": [
        "# Build a pooling operator with size `2`.\n",
        "max_pooling = torch.nn.MaxPool2d(2)\n",
        "\n",
        "# Apply the pooling operator\n",
        "output_feature = max_pooling(im)\n",
        "\n",
        "# Use pooling operator in the image\n",
        "output_feature_F = F.max_pool2d(im, 2)\n",
        "\n",
        "# print the results of both cases\n",
        "print(output_feature)\n",
        "print(output_feature_F)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[6., 9.],\n",
            "          [3., 4.]]]])\n",
            "tensor([[[[6., 9.],\n",
            "          [3., 4.]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di17evimxSJk"
      },
      "source": [
        "#Average-pooling operator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofgwiC6CwnJw",
        "outputId": "d59b18c4-0124-4ae9-bb40-d41c1e41a949"
      },
      "source": [
        "# Build a pooling operator with size `2`.\n",
        "avg_pooling = torch.nn.AvgPool2d(2)\n",
        "\n",
        "# Apply the pooling operator\n",
        "output_feature = avg_pooling(im)\n",
        "\n",
        "# Use pooling operator in the image\n",
        "output_feature_F = F.avg_pool2d(im, 2)\n",
        "\n",
        "# print the results of both cases\n",
        "print(output_feature)\n",
        "print(output_feature_F)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[2.5000, 6.0000],\n",
            "          [1.7500, 3.0000]]]])\n",
            "tensor([[[[2.5000, 6.0000],\n",
            "          [1.7500, 3.0000]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AldEKH0xb3f"
      },
      "source": [
        "#Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBD6AHCJ16Gv"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBmXCfwc19-f"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkbkuNXB2h2e"
      },
      "source": [
        "#Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBb-i0lz2Zx5",
        "outputId": "38077041-cb14-41cd-e2ea-d0741f4c7977"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.48216, 0.44543),\n",
        "                          (0.24703, 0.24349, 0.26519))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train = True, download = True, transform = transform)\n",
        "testset = torchvision.datasets.CIFAR10(root=\"./data\", train = False, download = True, transform = transform)\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle = True, num_workers = 4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle = False, num_workers = 4)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6brU77u52vOo"
      },
      "source": [
        "#Building a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaB8CUxH2kE3"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.fc = nn.Linear(128 * 4 * 4, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    x = x.view(-1, 128 * 4 * 4)\n",
        "    return self.fc(x)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WqN6XSgmu7j"
      },
      "source": [
        "#Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRJaB-2q3BeF"
      },
      "source": [
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgPcoJwnwo1"
      },
      "source": [
        "#Training a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lryXqCjnGU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffee857b-6f0a-4ab6-add5-05779d210ba9"
      },
      "source": [
        "for epoch in range(10):\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    #get the inputs\n",
        "    inputs, labels = data\n",
        "\n",
        "    #Zero the parameters the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()      #Compute the weights using backward()\n",
        "    optimizer.step()     #Update the weights using optimizer\n",
        "\n",
        "print('Finised Training')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finised Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtFD3j02pgVv"
      },
      "source": [
        "#Evaluating the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_sXY638ozvm"
      },
      "source": [
        "correct, total = 0,0\n",
        "predictions = []\n",
        "net.eval()\n",
        "\n",
        "for i, data in enumerate(testloader, 0):\n",
        "  inputs, labels = data\n",
        "  outputs = net(inputs)\n",
        "  _, predicted = torch.max(outputs.data, 1) # Argmax the results of the net\n",
        "  predictions.append(outputs)\n",
        "  total += label.size(0)\n",
        "  correct += (predicted ==labels).sum().item()\n",
        "\n",
        "print('The testing set accuracy of the network is: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42nH8KoTsXHK"
      },
      "source": [
        "#Excercise on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSAEW6DFsdrz"
      },
      "source": [
        "This time however, you will train the CNN you built in the previous lesson, instead of a fully connected network. The packages you need have been imported for you and the network (called net) instantiated. The cross-entropy loss function (called criterion) and the Adam optimizer (called optimizer) are also available. We have subsampled the training set so that the training goes faster, and you are going to use a single epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhUFJzOrsgW6"
      },
      "source": [
        "#Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J94FRdHkskd0"
      },
      "source": [
        "Compute the predictions from the net.\n",
        "\n",
        "Using the predictions and the labels, compute the loss function.\n",
        "\n",
        "Compute the gradients for each weight.\n",
        "\n",
        "Update the weights using the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvlAi6jUsrhQ"
      },
      "source": [
        "#Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG9-_lxSsw-7"
      },
      "source": [
        "criterion accepts the predicted values and the labels as arguments.\n",
        "\n",
        "Gradients can be computed via the backward() function.\n",
        "\n",
        "Weights can be updated via the step() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4s0g8knstnK"
      },
      "source": [
        "'''\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute the forward pass\n",
        "    outputs = net(inputs)\n",
        "        \n",
        "    # Compute the loss function\n",
        "    loss = criterion(outputs, labels)\n",
        "        \n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "        \n",
        "    # Update the weights\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REL6AVkPs95f"
      },
      "source": [
        "Building and training neural networks is a very exciting job (trust me, I do it every day)! However, the main utility of neural networks is to make predictions. This is the entire reason why the field of deep learning has bloomed in the last few years, as neural networks predictions are extremely accurate. On this exercise, we are going to use the convolutional neural network you already trained in order to make predictions on the MNIST dataset.\n",
        "\n",
        "Remember that torch.max() takes two arguments: -output.data - the tensor which contains the data.\n",
        "\n",
        "Either 1 to do argmax or 0 to do max."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aomnxNDMtI31"
      },
      "source": [
        "#Using CNNs to make predictions\n",
        "\n",
        "Building and training neural networks is a very exciting job (trust me, I do it every day)! However, the main utility of neural networks is to make predictions. This is the entire reason why the field of deep learning has bloomed in the last few years, as neural networks predictions are extremely accurate. On this exercise, we are going to use the convolutional neural network you already trained in order to make predictions on the MNIST dataset.\n",
        "\n",
        "Remember that torch.max() takes two arguments:\n",
        "\n",
        " - output.data \n",
        "\n",
        " - the tensor which contains the data.\n",
        "\n",
        " - Either 1 to do argmax or 0 to do max."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW_plpKNt0hy"
      },
      "source": [
        "Iterate over the given test_loader, saving the results of each iteration in data.\n",
        "\n",
        "Get the image and label from the data tuple, storing the results in image and label.\n",
        "\n",
        "Make a forward pass in the net using your image.\n",
        "\n",
        "Get the net prediction using torch.max() function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK_CSO4atCJs"
      },
      "source": [
        "'''\n",
        "# Iterate over the data in the test_loader\n",
        "for i, data in enumerate(test_loader):\n",
        "\n",
        "    # Get the image and label from data\n",
        "    image, label = data\n",
        "\n",
        "    # Make a forward pass in the net with your image\n",
        "    output = net(image)\n",
        "\n",
        "    # Argmax the results of the net\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    if predicted == label:\n",
        "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
        "    else:\n",
        "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + str(label))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}